â”‚   â””â”€â”€ analytics.py       # Analytics & IA avec LLM
# Web Analytics Project - Application Crawler AvancÃ©e

Une application **FastAPI** complÃ¨te pour la collecte, le traitement et l'analyse de donnÃ©es web avec support avancÃ© pour le scraping, les flux RSS et les rÃ©seaux sociaux.

## ğŸ“‹ Table des matiÃ¨res
- [Vue d'ensemble](#vue-densemble)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [Architecture](#architecture)
- [Installation](#installation)
- [Configuration](#configuration)
- [Docker](#docker)
- [Utilisation](#utilisation)
- [API Endpoints](#api-endpoints)
- [Tests](#tests)

---

## ğŸ¯ Vue d'ensemble

Cette application est un **systÃ¨me complet de web scraping** avec les capacitÃ©s suivantes:

| Phase | Description | Statut |
|-------|-------------|--------|
| **Phase 1** | Collecte de donnÃ©es et recherche par mots-clÃ©s | âœ… Complet |
| **Phase 2** | Programmation automatique des tÃ¢ches | âœ… Complet |
| **Phase 3a** | Support des flux RSS | âœ… Complet |
| **Phase 3b** | IntÃ©gration rÃ©seaux sociaux | âœ… Complet |
| **Phase 4** | Containerisation Docker | âœ… Complet |
| **Phase 5** | Analytics & IA avec LLM | âœ… Complet |

---

## âœ¨ FonctionnalitÃ©s

### ğŸ”· Phase 1: Collecte de DonnÃ©es

**Scraping Web AvancÃ©**
- Scrape multiple formats: HTML, XML, PDF, CSV, TXT
- Extraction intelligente du contenu avec **BeautifulSoup4**
- Support des sÃ©lecteurs CSS personnalisÃ©s
- Gestion des timeout et des erreurs

**Gestion des Sources**
- Enregistrement et gestion des sources de scraping
- CRUD complet: CrÃ©er, Lire, Mettre Ã  jour, Supprimer
- Validation Pydantic V2 des donnÃ©es
- MÃ©tadonnÃ©es enrichies (catÃ©gorie, tags, etc.)

**Recherche par Mots-clÃ©s**
- Recherche regex flexible dans les documents scraped
- Filtrage par date
- Support de la casse sensible/insensible
- Pagination des rÃ©sultats

### ğŸ”· Phase 2: Planification Automatique

**Scheduler APScheduler**
- Programmation automatique des tÃ¢ches de scraping
- DÃ©clenchement par intervalle configurable
- Gestion des jobs: dÃ©marrage, arrÃªt, reschÃ©dulage
- PersÃ©vÃ©rance: les jobs survivent aux redÃ©marrages via MongoDB
- Monitoring et statistiques des tÃ¢ches

**Configuration Globale**
- FrÃ©quence globale de scraping ajustable
- Limite de hits par source
- Timeout configurable
- Gestion des tentatives (retry)

### ğŸ”· Phase 3a: Support RSS

**Parseur RSS**
- Parsing de flux RSS/Atom avec **feedparser**
- Extraction automatique des entrÃ©es
- Scraping du contenu de chaque article
- Stockage avec mÃ©tadonnÃ©es (auteur, date, categorie)

**Endpoints RSS**
- Parse-feed: Analyser un flux RSS
- Scrape-rss: RÃ©cupÃ©rer et stocker les articles
- Add-source: Enregistrer une source RSS
- Get-sources: Lister les sources RSS
- Get-latest: RÃ©cupÃ©rer les articles rÃ©cents
- Refresh: Mettre Ã  jour manuellement un flux

### ğŸ”· Phase 3b: IntÃ©gration RÃ©seaux Sociaux

**Support Multi-plateforme**
- **Twitter/X**: Tweets et recherches
- **Instagram**: Posts et hashtags
- **Facebook**: Contenu public et pages
- **LinkedIn**: Posts professionnels et articles

**FonctionnalitÃ©s Sociales**
- Connexion Ã  chaque plateforme
- Scraping de contenu public
- RÃ©cupÃ©ration de statistiques (likes, shares, etc.)
- Tests de connexion Ã  chaque source
- Stockage du contenu avec mÃ©tadonnÃ©es complÃ¨tes

### ğŸ”· Phase 5: Analytics & Intelligence Artificielle

**Analyse par LLM (OpenAI GPT)**
- Analyse automatique des documents scrapÃ©s
- RÃ©sumÃ© intelligent de contenu
- DÃ©tection de sentiment (positif, nÃ©gatif, neutre)
- Classification automatique par catÃ©gories
- Extraction de mots-clÃ©s pertinents
- Reconnaissance d'entitÃ©s nommÃ©es (personnes, lieux, organisations)

**Visualisations & Dashboards**
- Distribution des sentiments (graphiques en barres)
- RÃ©partition par catÃ©gories
- Top 20 mots-clÃ©s avec taille proportionnelle
- Recherche sÃ©mantique par mots-clÃ©s extraits
- Filtrage par catÃ©gorie

**Endpoints Analytics**
- POST /analytics/analyze-document: Analyser un document spÃ©cifique
- POST /analytics/analyze-batch: Analyser plusieurs documents en batch
- GET /analytics/stats: Statistiques globales d'analyse
- GET /analytics/documents-by-category/{category}: Documents par catÃ©gorie
- GET /analytics/search-by-keywords: Recherche sÃ©mantique

---

## ğŸ—ï¸ Architecture

### Structure du Projet

```
webanalproject/
â”œâ”€â”€ main.py                 # Point d'entrÃ©e FastAPI
â”œâ”€â”€ db.py                   # Connexion MongoDB
â”œâ”€â”€ models.py               # ModÃ¨les Pydantic
â”œâ”€â”€ scheduler.py            # Gestion APScheduler
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ .env.example            # Template de configuration
â”‚
â”œâ”€â”€ routes/                 # Modules de fonctionnalitÃ©s
â”‚   â”œâ”€â”€ scrape.py          # Scraping web
â”‚   â”œâ”€â”€ sources.py         # CRUD sources
â”‚   â”œâ”€â”€ config.py          # Configuration globale
â”‚   â”œâ”€â”€ search.py          # Recherche par mots-clÃ©s
â”‚   â”œâ”€â”€ scheduler_routes.py # Endpoints scheduler
â”‚   â”œâ”€â”€ rss.py             # Gestion des flux RSS
â”‚   â””â”€â”€ social_media.py    # IntÃ©gration rÃ©seaux sociaux
â”‚
â”œâ”€â”€ frontend/              # Interface utilisateur
â”œâ”€â”€ Dockerfile             # Containerisation
â”œâ”€â”€ docker-compose.yml     # Environnement dev
â””â”€â”€ docker-compose.prod.yml # Configuration production
```

### Stack Technologique

| Composant | Technologie | Version |
|-----------|-------------|---------|
| Framework Web | FastAPI | 0.128.0 |
| Serveur | Uvicorn | 0.40.0 |
| Base de donnÃ©es | MongoDB | 7.0+ |
| Scraping | BeautifulSoup4 | 4.14.3 |
| PDF | pypdf | 6.6.0 |
| RSS | feedparser | - |
| Scheduling | APScheduler | 3.11.2 |
| Validation | Pydantic | 2.12.5 |
| LLM | OpenAI GPT | 2.15.0 |
| Tests | pytest | 9.0.2 |
| Python | 3.13.9 | - |

---

## ğŸ“¦ Installation

### PrÃ©requis
- Python 3.13+
- MongoDB 7.0+
- pip

### Ã‰tapes d'installation

1. **Cloner le projet**
```bash
git clone <repo-url>
cd webanalproject
```

2. **CrÃ©er un environnement virtuel**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

4. **Configurer les variables d'environnement**
```bash
# Copier le template
cp .env.example .env

# Ã‰diter .env avec vos paramÃ¨tres
# - MONGO_URI: mongodb://user:pass@host:port/db
# - MONGODB_DB: nom de la base de donnÃ©es
# - OPENAI_API_KEY: clÃ© API OpenAI pour l'analyse IA
# - API_HOST: localhost
# - API_PORT: 8000
```

5. **Lancer l'application**

**Option A: Sans Docker (dÃ©veloppement)**
```bash
# Backend
uvicorn main:app --reload --port 8000

# Frontend (nouveau terminal)
cd frontend
npm install
npm run dev
```

**Option B: Avec Docker (production)**
```bash
# Lancer tous les services (backend + frontend + MongoDB)
docker-compose up -d

# VÃ©rifier les logs
docker-compose logs -f

# ArrÃªter les services
docker-compose down
```

L'API sera accessible sur http://localhost:8000 et le frontend sur http://localhost (ou http://localhost:5173 en dev).

---

## ğŸ³ Docker

### DÃ©marrage Rapide

**Lancer toute l'application avec une seule commande:**
```bash
docker-compose up -d
```

Cela lance:
- **MongoDB** sur le port 27017
- **Backend API** sur le port 8000
- **Frontend** sur le port 80

### Services Disponibles

| Service | URL | Description |
|---------|-----|-------------|
| Frontend | http://localhost | Interface React |
| Backend API | http://localhost:8000 | API FastAPI |
| API Docs | http://localhost:8000/docs | Swagger UI |
| MongoDB | localhost:27017 | Base de donnÃ©es |

### Commandes Docker Utiles

```bash
# DÃ©marrer les services
docker-compose up -d

# Voir les logs
docker-compose logs -f backend
docker-compose logs -f frontend

# ArrÃªter les services
docker-compose down

# Rebuild aprÃ¨s modifications
docker-compose up -d --build

# Voir les services actifs
docker-compose ps

# AccÃ©der au conteneur backend
docker exec -it webanalproject_backend bash

# Nettoyer tout (containers + volumes)
docker-compose down -v
```

### Variables d'Environnement Docker

CrÃ©er un fichier `.env` Ã  la racine:
```env
# MongoDB (utilisÃ© par docker-compose)
MONGO_URI=mongodb://admin:admin123@mongodb:27017/

# OpenAI
OPENAI_API_KEY=sk-your-api-key-here

# Frontend
VITE_API_BASE_URL=http://localhost:8000
```

### Architecture Docker

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend (Nginx)            â”‚
â”‚       Port: 80                      â”‚
â”‚       Build: React + Vite           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Backend API (FastAPI)          â”‚
â”‚       Port: 8000                    â”‚
â”‚       Python 3.11                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       MongoDB Database              â”‚
â”‚       Port: 27017                   â”‚
â”‚       Version: 7.0                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Healthchecks

Tous les services ont des healthchecks intÃ©grÃ©s:
- Backend: `GET /health`
- MongoDB: `mongosh ping`
- Frontend: nginx status



---

## âš™ï¸ Configuration

### Variables d'environnement (.env)

```env
# MongoDB
MONGO_URI=mongodb://localhost:27017/web_analytics
MONGODB_DB=web_analytics

# API
API_HOST=localhost
API_PORT=8000

# Scraping
SCRAPE_TIMEOUT=10
MAX_RETRIES=3

# Scheduler
SCHEDULER_INTERVAL=3600  # 1 heure par dÃ©faut

# OpenAI (pour Analytics & IA)
OPENAI_API_KEY=sk-...  # Votre clÃ© API OpenAI
```

### Configuration du Crawler (via API)

```bash
# RÃ©cupÃ©rer la configuration actuelle
curl http://localhost:8000/config

# Mettre Ã  jour la configuration
curl -X PUT http://localhost:8000/config \
  -H "Content-Type: application/json" \
  -d '{
    "global_frequency": 3600,
    "max_hits_per_source": 100,
    "timeout": 10,
    "retry_count": 3
  }'
```

---

## ğŸš€ Utilisation

### Exemple complet d'utilisation

#### 1. CrÃ©er une source
```bash
curl -X POST http://localhost:8000/sources \
  -H "Content-Type: application/json" \
  -d '{
    "name": "BBC News",
    "url": "https://www.bbc.com/news",
    "category": "news",
    "tags": ["news", "politics"]
  }'
```

#### 2. Scraper une URL
```bash
curl -X POST http://localhost:8000/scrape/manual \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "selector": "article",
    "content_type": "html"
  }'
```

#### 3. Ajouter un flux RSS
```bash
curl -X POST http://localhost:8000/rss/add-source \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Tech News RSS",
    "url": "https://example.com/feed.xml",
    "category": "technology"
  }'
```

#### 4. Scraper un flux RSS
```bash
curl -X POST http://localhost:8000/rss/scrape-rss \
  -H "Content-Type: application/json" \
  -d '{
    "rss_url": "https://example.com/feed.xml"
  }'
```

#### 5. Ajouter une source rÃ©seaux sociaux
```bash
curl -X POST http://localhost:8000/social/add-source \
  -H "Content-Type: application/json" \
  -d '{
    "platform": "twitter",
    "handle": "@username",
    "api_key": "your-api-key"
  }'
```

#### 6. Rechercher du contenu
```bash
curl "http://localhost:8000/search?keyword=python&case_sensitive=false&limit=50"
```

#### 7. Planifier un scraping automatique
```bash
curl -X POST http://localhost:8000/scheduler/schedule \
  -H "Content-Type: application/json" \
  -d '{
    "source_id": "507f1f77bcf86cd799439011",
    "interval_seconds": 3600
  }'
```

---

## ğŸ“¡ API Endpoints

### Scraping (`/scrape`)
| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/scrape/manual` | Scraper une URL donnÃ©e |
| POST | `/scrape/by-source` | Scraper une source enregistrÃ©e |
| GET | `/scrape/health` | VÃ©rifier l'Ã©tat de l'API |

### Sources (`/sources`)
| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/sources` | CrÃ©er une nouvelle source |
| GET | `/sources` | Lister toutes les sources |
| GET | `/sources/{id}` | RÃ©cupÃ©rer une source |
| PUT | `/sources/{id}` | Mettre Ã  jour une source |
| DELETE | `/sources/{id}` | Supprimer une source |

### Configuration (`/config`)
| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/config` | RÃ©cupÃ©rer la configuration |
| PUT | `/config` | Mettre Ã  jour la configuration |
| GET | `/config/stats` | Statistiques du crawler |
| POST | `/config/toggle-pause` | Pause/Reprendre le crawler |

### Recherche (`/search`)
| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/search` | Rechercher par mot-clÃ© |

### Scheduler (`/scheduler`)
| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/scheduler/schedule` | Planifier une tÃ¢che |
| POST | `/scheduler/unschedule` | ArrÃªter une tÃ¢che planifiÃ©e |
| POST | `/scheduler/reschedule-all` | Re-planifier toutes les sources |
| GET | `/scheduler/jobs` | Lister les jobs actifs |
| POST | `/scheduler/start` | DÃ©marrer le scheduler |
| POST | `/scheduler/stop` | ArrÃªter le scheduler |
| GET | `/scheduler/status` | Ã‰tat du scheduler |

### RSS (`/rss`)
| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/rss/parse` | Parser un flux RSS |
| POST | `/rss/scrape-rss` | Scraper un flux RSS |
| POST | `/rss/add-source` | Enregistrer une source RSS |
| GET | `/rss/get-sources` | Lister les sources RSS |
| GET | `/rss/get-latest` | Articles RSS rÃ©cents |
| POST | `/rss/refresh` | Mettre Ã  jour un flux |

### RÃ©seaux Sociaux (`/social`)
| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/social/add-source` | Ajouter une source sociales |
| GET | `/social/get-sources` | Lister les sources sociales |
| POST | `/social/test-connection` | Tester une connexion |
| POST | `/social/scrape` | Scraper du contenu social |
| GET | `/social/get-posts` | RÃ©cupÃ©rer les posts |
| GET | `/social/stats` | Statistiques sociales |

---

## ğŸ³ Docker

### Lancer avec Docker Compose (DÃ©veloppement)

```bash
# DÃ©marrer les services
docker-compose up -d

# VÃ©rifier l'Ã©tat
docker-compose ps

# Afficher les logs
docker-compose logs -f api

# ArrÃªter les services
docker-compose down
```

L'API sera accessible sur `http://localhost:8000`

### Lancer en Production

```bash
# Avec docker-compose.prod.yml
docker-compose -f docker-compose.prod.yml up -d

# VÃ©rifier la santÃ©
curl http://localhost:8000/scrape/health

# Afficher les logs JSON
docker-compose -f docker-compose.prod.yml logs api
```

### Build personnalisÃ©

```bash
# Construire l'image
docker build -t web-analytics:latest .

# Lancer le container
docker run -d \
  --name analytics \
  -p 8000:8000 \
  -e MONGO_URI=mongodb://mongo:27017/web_analytics \
  web-analytics:latest

# AccÃ©der aux logs
docker logs analytics
```

---

## ğŸ§ª Tests

Tous les tests unitaires et d'intÃ©gration sont inclus.

```bash
# Lancer tous les tests
pytest -v

# Tests par module
pytest test_sources.py -v      # Sources CRUD
pytest test_config.py -v       # Configuration
pytest test_search.py -v       # Recherche
pytest test_scheduler.py -v    # Scheduler
pytest test_rss.py -v          # RSS
pytest test_social_media.py -v # RÃ©seaux sociaux

# Avec rapport de couverture
pytest --cov=routes --cov=scheduler --cov=db
```

---

## ğŸ“Š Base de donnÃ©es

### Collections MongoDB

| Collection | Description |
|-----------|-------------|
| `scraped_data` | Contenu scraped avec mÃ©tadonnÃ©es |
| `sources` | Sources de scraping enregistrÃ©es |
| `rss_sources` | Sources de flux RSS |
| `rss_entries` | Articles RSS parsÃ©s |
| `social_sources` | Sources rÃ©seaux sociaux |
| `social_posts` | Posts sociaux collectÃ©s |
| `crawler_config` | Configuration globale du crawler |
| `scheduler_jobs` | Historique des jobs scheduler |

---

## ğŸ” SÃ©curitÃ©

- âœ… Variables d'environnement sÃ©curisÃ©es (pas de hardcoding)
- âœ… MongoDB avec authentification
- âœ… Validation Pydantic V2 stricte
- âœ… User non-root dans Docker
- âœ… Health checks intÃ©grÃ©s
- âœ… Gestion des erreurs complÃ¨te

---

## ğŸ“š Documentation supplÃ©mentaire

- [Swagger API](http://localhost:8000/docs) - Documentation interactive
- [ReDoc](http://localhost:8000/redoc) - Documentation alternative
- [systemdesign.md](systemdesign.md) - Architecture dÃ©taillÃ©e
- [tests/README.md](tests/README.md) - Documentation des tests

---

## ğŸ§ª Tests

Tous les tests sont organisÃ©s dans le dossier `tests/`.

### Lancer les tests

```bash
# Tous les tests
pytest tests/

# Tests spÃ©cifiques
pytest tests/test_main.py
pytest tests/test_scheduler.py

# Avec couverture
pytest tests/ --cov=. --cov-report=html

# Mode verbose
pytest tests/ -v
```

### Structure des tests

- `test_main.py` - Tests de scraping HTML
- `test_config.py` - Tests de configuration
- `test_search.py` - Tests de recherche
- `test_sources.py` - Tests CRUD sources
- `test_scheduler.py` - Tests du scheduler
- `test_rss.py` - Tests flux RSS
- `test_social_media.py` - Tests rÃ©seaux sociaux
- `test_integration.py` - Tests d'intÃ©gration

Voir [tests/README.md](tests/README.md) pour plus de dÃ©tails.

---

## ğŸ“š Documentation supplÃ©mentaire

- [Swagger API](http://localhost:8000/docs) - Documentation interactive
- [ReDoc](http://localhost:8000/redoc) - Documentation alternative
- [systemdesign.md](systemdesign.md) - Architecture dÃ©taillÃ©e
- [tests/README.md](tests/README.md) - Documentation des tests

---

## ğŸ¤ Support

Pour les problÃ¨mes ou questions:
1. Consulter les logs: `docker-compose logs api`
2. VÃ©rifier la configuration: `curl http://localhost:8000/config`
3. VÃ©rifier la santÃ©: `curl http://localhost:8000/scrape/health`

---

## ğŸ“ Licence

MIT

---

**DerniÃ¨re mise Ã  jour**: Janvier 2026 | **Version**: 5.0 (Analytics & IA avec LLM)
