# Web Analytics Project - Application Crawler Avanc√©e

Une application **FastAPI** compl√®te pour la collecte, le traitement et l'analyse de donn√©es web avec support avanc√© pour le scraping, les flux RSS et les r√©seaux sociaux.

## üìã Table des mati√®res
- [Vue d'ensemble](#vue-densemble)
- [Fonctionnalit√©s](#fonctionnalit√©s)
- [Architecture](#architecture)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [API Endpoints](#api-endpoints)
- [Docker](#docker)

---

## üéØ Vue d'ensemble

Cette application est un **syst√®me complet de web scraping** avec les capacit√©s suivantes:

| Phase | Description | Statut |
|-------|-------------|--------|
| **Phase 1** | Collecte de donn√©es et recherche par mots-cl√©s | ‚úÖ Complet |
| **Phase 2** | Programmation automatique des t√¢ches | ‚úÖ Complet |
| **Phase 3a** | Support des flux RSS | ‚úÖ Complet |
| **Phase 3b** | Int√©gration r√©seaux sociaux | ‚úÖ Complet |
| **Phase 4** | Containerisation Docker | ‚úÖ Complet |

---

## ‚ú® Fonctionnalit√©s

### üî∑ Phase 1: Collecte de Donn√©es

**Scraping Web Avanc√©**
- Scrape multiple formats: HTML, XML, PDF, CSV, TXT
- Extraction intelligente du contenu avec **BeautifulSoup4**
- Support des s√©lecteurs CSS personnalis√©s
- Gestion des timeout et des erreurs

**Gestion des Sources**
- Enregistrement et gestion des sources de scraping
- CRUD complet: Cr√©er, Lire, Mettre √† jour, Supprimer
- Validation Pydantic V2 des donn√©es
- M√©tadonn√©es enrichies (cat√©gorie, tags, etc.)

**Recherche par Mots-cl√©s**
- Recherche regex flexible dans les documents scraped
- Filtrage par date
- Support de la casse sensible/insensible
- Pagination des r√©sultats

### üî∑ Phase 2: Planification Automatique

**Scheduler APScheduler**
- Programmation automatique des t√¢ches de scraping
- D√©clenchement par intervalle configurable
- Gestion des jobs: d√©marrage, arr√™t, resch√©dulage
- Pers√©v√©rance: les jobs survivent aux red√©marrages via MongoDB
- Monitoring et statistiques des t√¢ches

**Configuration Globale**
- Fr√©quence globale de scraping ajustable
- Limite de hits par source
- Timeout configurable
- Gestion des tentatives (retry)

### üî∑ Phase 3a: Support RSS

**Parseur RSS**
- Parsing de flux RSS/Atom avec **feedparser**
- Extraction automatique des entr√©es
- Scraping du contenu de chaque article
- Stockage avec m√©tadonn√©es (auteur, date, categorie)

**Endpoints RSS**
- Parse-feed: Analyser un flux RSS
- Scrape-rss: R√©cup√©rer et stocker les articles
- Add-source: Enregistrer une source RSS
- Get-sources: Lister les sources RSS
- Get-latest: R√©cup√©rer les articles r√©cents
- Refresh: Mettre √† jour manuellement un flux

### üî∑ Phase 3b: Int√©gration R√©seaux Sociaux

**Support Multi-plateforme**
- **Twitter/X**: Tweets et recherches
- **Instagram**: Posts et hashtags
- **Facebook**: Contenu public et pages
- **LinkedIn**: Posts professionnels et articles

**Fonctionnalit√©s Sociales**
- Connexion √† chaque plateforme
- Scraping de contenu public
- R√©cup√©ration de statistiques (likes, shares, etc.)
- Tests de connexion √† chaque source
- Stockage du contenu avec m√©tadonn√©es compl√®tes

---

## üèóÔ∏è Architecture

### Structure du Projet

```
webanalproject/
‚îú‚îÄ‚îÄ main.py                 # Point d'entr√©e FastAPI
‚îú‚îÄ‚îÄ db.py                   # Connexion MongoDB
‚îú‚îÄ‚îÄ models.py               # Mod√®les Pydantic
‚îú‚îÄ‚îÄ scheduler.py            # Gestion APScheduler
‚îú‚îÄ‚îÄ requirements.txt        # D√©pendances Python
‚îú‚îÄ‚îÄ .env.example            # Template de configuration
‚îÇ
‚îú‚îÄ‚îÄ routes/                 # Modules de fonctionnalit√©s
‚îÇ   ‚îú‚îÄ‚îÄ scrape.py          # Scraping web
‚îÇ   ‚îú‚îÄ‚îÄ sources.py         # CRUD sources
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration globale
‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Recherche par mots-cl√©s
‚îÇ   ‚îú‚îÄ‚îÄ scheduler_routes.py # Endpoints scheduler
‚îÇ   ‚îú‚îÄ‚îÄ rss.py             # Gestion des flux RSS
‚îÇ   ‚îî‚îÄ‚îÄ social_media.py    # Int√©gration r√©seaux sociaux
‚îÇ
‚îú‚îÄ‚îÄ frontend/              # Interface utilisateur
‚îú‚îÄ‚îÄ Dockerfile             # Containerisation
‚îú‚îÄ‚îÄ docker-compose.yml     # Environnement dev
‚îî‚îÄ‚îÄ docker-compose.prod.yml # Configuration production
```

### Stack Technologique

| Composant | Technologie | Version |
|-----------|-------------|---------|
| Framework Web | FastAPI | 0.128.0 |
| Serveur | Uvicorn | 0.40.0 |
| Base de donn√©es | MongoDB | 7.0+ |
| Scraping | BeautifulSoup4 | 4.14.3 |
| PDF | pypdf | 6.6.0 |
| RSS | feedparser | - |
| Scheduling | APScheduler | 3.11.2 |
| Validation | Pydantic | 2.12.5 |
| Tests | pytest | 9.0.2 |
| Python | 3.13.9 | - |

---

## üì¶ Installation

### Pr√©requis
- Python 3.13+
- MongoDB 7.0+
- pip

### √âtapes d'installation

1. **Cloner le projet**
```bash
git clone <repo-url>
cd webanalproject
```

2. **Cr√©er un environnement virtuel**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3. **Installer les d√©pendances**
```bash
pip install -r requirements.txt
```

4. **Configurer les variables d'environnement**
```bash
# Copier le template
cp .env.example .env

# √âditer .env avec vos param√®tres
# - MONGO_URI: mongodb://user:pass@host:port/db
# - MONGODB_DB: nom de la base de donn√©es
# - API_HOST: localhost
# - API_PORT: 8000
```

5. **Lancer l'application**
```bash
python main.py
# L'API sera accessible sur http://localhost:8000
```

---

## ‚öôÔ∏è Configuration

### Variables d'environnement (.env)

```env
# MongoDB
MONGO_URI=mongodb://localhost:27017/web_analytics
MONGODB_DB=web_analytics

# API
API_HOST=localhost
API_PORT=8000

# Scraping
SCRAPE_TIMEOUT=10
MAX_RETRIES=3

# Scheduler
SCHEDULER_INTERVAL=3600  # 1 heure par d√©faut
```

### Configuration du Crawler (via API)

```bash
# R√©cup√©rer la configuration actuelle
curl http://localhost:8000/config

# Mettre √† jour la configuration
curl -X PUT http://localhost:8000/config \
  -H "Content-Type: application/json" \
  -d '{
    "global_frequency": 3600,
    "max_hits_per_source": 100,
    "timeout": 10,
    "retry_count": 3
  }'
```

---

## üöÄ Utilisation

### Exemple complet d'utilisation

#### 1. Cr√©er une source
```bash
curl -X POST http://localhost:8000/sources \
  -H "Content-Type: application/json" \
  -d '{
    "name": "BBC News",
    "url": "https://www.bbc.com/news",
    "category": "news",
    "tags": ["news", "politics"]
  }'
```

#### 2. Scraper une URL
```bash
curl -X POST http://localhost:8000/scrape/manual \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "selector": "article",
    "content_type": "html"
  }'
```

#### 3. Ajouter un flux RSS
```bash
curl -X POST http://localhost:8000/rss/add-source \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Tech News RSS",
    "url": "https://example.com/feed.xml",
    "category": "technology"
  }'
```

#### 4. Scraper un flux RSS
```bash
curl -X POST http://localhost:8000/rss/scrape-rss \
  -H "Content-Type: application/json" \
  -d '{
    "rss_url": "https://example.com/feed.xml"
  }'
```

#### 5. Ajouter une source r√©seaux sociaux
```bash
curl -X POST http://localhost:8000/social/add-source \
  -H "Content-Type: application/json" \
  -d '{
    "platform": "twitter",
    "handle": "@username",
    "api_key": "your-api-key"
  }'
```

#### 6. Rechercher du contenu
```bash
curl "http://localhost:8000/search?keyword=python&case_sensitive=false&limit=50"
```

#### 7. Planifier un scraping automatique
```bash
curl -X POST http://localhost:8000/scheduler/schedule \
  -H "Content-Type: application/json" \
  -d '{
    "source_id": "507f1f77bcf86cd799439011",
    "interval_seconds": 3600
  }'
```

---

## üì° API Endpoints

### Scraping (`/scrape`)
| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/scrape/manual` | Scraper une URL donn√©e |
| POST | `/scrape/by-source` | Scraper une source enregistr√©e |
| GET | `/scrape/health` | V√©rifier l'√©tat de l'API |

### Sources (`/sources`)
| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/sources` | Cr√©er une nouvelle source |
| GET | `/sources` | Lister toutes les sources |
| GET | `/sources/{id}` | R√©cup√©rer une source |
| PUT | `/sources/{id}` | Mettre √† jour une source |
| DELETE | `/sources/{id}` | Supprimer une source |

### Configuration (`/config`)
| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/config` | R√©cup√©rer la configuration |
| PUT | `/config` | Mettre √† jour la configuration |
| GET | `/config/stats` | Statistiques du crawler |
| POST | `/config/toggle-pause` | Pause/Reprendre le crawler |

### Recherche (`/search`)
| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/search` | Rechercher par mot-cl√© |

### Scheduler (`/scheduler`)
| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/scheduler/schedule` | Planifier une t√¢che |
| POST | `/scheduler/unschedule` | Arr√™ter une t√¢che planifi√©e |
| POST | `/scheduler/reschedule-all` | Re-planifier toutes les sources |
| GET | `/scheduler/jobs` | Lister les jobs actifs |
| POST | `/scheduler/start` | D√©marrer le scheduler |
| POST | `/scheduler/stop` | Arr√™ter le scheduler |
| GET | `/scheduler/status` | √âtat du scheduler |

### RSS (`/rss`)
| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/rss/parse` | Parser un flux RSS |
| POST | `/rss/scrape-rss` | Scraper un flux RSS |
| POST | `/rss/add-source` | Enregistrer une source RSS |
| GET | `/rss/get-sources` | Lister les sources RSS |
| GET | `/rss/get-latest` | Articles RSS r√©cents |
| POST | `/rss/refresh` | Mettre √† jour un flux |

### R√©seaux Sociaux (`/social`)
| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/social/add-source` | Ajouter une source sociales |
| GET | `/social/get-sources` | Lister les sources sociales |
| POST | `/social/test-connection` | Tester une connexion |
| POST | `/social/scrape` | Scraper du contenu social |
| GET | `/social/get-posts` | R√©cup√©rer les posts |
| GET | `/social/stats` | Statistiques sociales |

---

## üê≥ Docker

### Lancer avec Docker Compose (D√©veloppement)

```bash
# D√©marrer les services
docker-compose up -d

# V√©rifier l'√©tat
docker-compose ps

# Afficher les logs
docker-compose logs -f api

# Arr√™ter les services
docker-compose down
```

L'API sera accessible sur `http://localhost:8000`

### Lancer en Production

```bash
# Avec docker-compose.prod.yml
docker-compose -f docker-compose.prod.yml up -d

# V√©rifier la sant√©
curl http://localhost:8000/scrape/health

# Afficher les logs JSON
docker-compose -f docker-compose.prod.yml logs api
```

### Build personnalis√©

```bash
# Construire l'image
docker build -t web-analytics:latest .

# Lancer le container
docker run -d \
  --name analytics \
  -p 8000:8000 \
  -e MONGO_URI=mongodb://mongo:27017/web_analytics \
  web-analytics:latest

# Acc√©der aux logs
docker logs analytics
```

---

## üß™ Tests

Tous les tests unitaires et d'int√©gration sont inclus.

```bash
# Lancer tous les tests
pytest -v

# Tests par module
pytest test_sources.py -v      # Sources CRUD
pytest test_config.py -v       # Configuration
pytest test_search.py -v       # Recherche
pytest test_scheduler.py -v    # Scheduler
pytest test_rss.py -v          # RSS
pytest test_social_media.py -v # R√©seaux sociaux

# Avec rapport de couverture
pytest --cov=routes --cov=scheduler --cov=db
```

---

## üìä Base de donn√©es

### Collections MongoDB

| Collection | Description |
|-----------|-------------|
| `scraped_data` | Contenu scraped avec m√©tadonn√©es |
| `sources` | Sources de scraping enregistr√©es |
| `rss_sources` | Sources de flux RSS |
| `rss_entries` | Articles RSS pars√©s |
| `social_sources` | Sources r√©seaux sociaux |
| `social_posts` | Posts sociaux collect√©s |
| `crawler_config` | Configuration globale du crawler |
| `scheduler_jobs` | Historique des jobs scheduler |

---

## üîê S√©curit√©

- ‚úÖ Variables d'environnement s√©curis√©es (pas de hardcoding)
- ‚úÖ MongoDB avec authentification
- ‚úÖ Validation Pydantic V2 stricte
- ‚úÖ User non-root dans Docker
- ‚úÖ Health checks int√©gr√©s
- ‚úÖ Gestion des erreurs compl√®te

---

## üìö Documentation suppl√©mentaire

- [Swagger API](http://localhost:8000/docs) - Documentation interactive
- [ReDoc](http://localhost:8000/redoc) - Documentation alternative
- [systemdesign.md](systemdesign.md) - Architecture d√©taill√©e

---

## ü§ù Support

Pour les probl√®mes ou questions:
1. Consulter les logs: `docker-compose logs api`
2. V√©rifier la configuration: `curl http://localhost:8000/config`
3. V√©rifier la sant√©: `curl http://localhost:8000/scrape/health`

---

## üìù Licence

MIT

---

**Derni√®re mise √† jour**: Janvier 2026 | **Version**: 4.0 (Phase 4 compl√®te)
